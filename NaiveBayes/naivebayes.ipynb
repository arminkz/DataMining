{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Train Data ...\n",
      "OK\n",
      "Reading Test Data ...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "#import dataset vocab\n",
    "f = open('imdb_dataset/imdb.vocab', 'r')\n",
    "\n",
    "word2id = {}\n",
    "words = []\n",
    "\n",
    "id = 0\n",
    "for line in f:\n",
    "    line = line.rstrip('\\n')\n",
    "    words.append(line)\n",
    "    word2id[line] = id\n",
    "    id += 1\n",
    "    \n",
    "def getID(word):\n",
    "    if not word in word2id:\n",
    "        return -1;\n",
    "    return word2id[word]\n",
    "\n",
    "#store in sparse CSR matrix\n",
    "print(\"Reading Train Data ...\")\n",
    "X_reviews = []\n",
    "X_values = []\n",
    "X_row_indices = []\n",
    "X_col_indices = []\n",
    "Y = []\n",
    "\n",
    "pFiles = os.listdir(\"imdb_dataset/train/pos\")\n",
    "nFiles = os.listdir(\"imdb_dataset/train/neg\")\n",
    "\n",
    "#Read positive files\n",
    "for i in range(len(pFiles)):\n",
    "    f = pFiles[i]\n",
    "    lines = \"\"\n",
    "    for line in open(\"imdb_dataset/train/pos/%s\" % f, encoding=\"utf8\"):\n",
    "        lines += line\n",
    "        wordCounts = Counter([getID(w.lower()) for w in line.split(\" \")])\n",
    "        for (wordId, count) in wordCounts.items():\n",
    "            if wordId >= 0:\n",
    "                X_row_indices.append(i)\n",
    "                X_col_indices.append(wordId)\n",
    "                X_values.append(count)\n",
    "    Y.append(+1.0)\n",
    "    X_reviews.append(lines)\n",
    "    \n",
    "#Read negative files\n",
    "for i in range(len(nFiles)):\n",
    "    f = nFiles[i]\n",
    "    lines = \"\"\n",
    "    for line in open(\"imdb_dataset/train/neg/%s\" % f, encoding=\"utf8\"):\n",
    "        lines += line\n",
    "        wordCounts = Counter([getID(w.lower()) for w in line.split(\" \")])\n",
    "        for (wordId, count) in wordCounts.items():\n",
    "            if wordId >= 0:\n",
    "                X_row_indices.append(len(pFiles)+i)\n",
    "                X_col_indices.append(wordId)\n",
    "                X_values.append(count)\n",
    "    Y.append(-1.0)\n",
    "    X_reviews.append(lines)\n",
    "    \n",
    "#Create a sparse matrix in csr format\n",
    "X = csr_matrix((X_values, (X_row_indices, X_col_indices)), shape=(max(X_row_indices)+1, len(words)))        \n",
    "Y = np.asarray(Y)\n",
    "\n",
    "#Randomly shuffle\n",
    "index = np.arange(X.shape[0])\n",
    "np.random.shuffle(index)\n",
    "X_train = X[index,:]\n",
    "Y_train = Y[index]\n",
    "\n",
    "print(\"OK\")\n",
    "#store in sparse CSR matrix\n",
    "print(\"Reading Test Data ...\")\n",
    "X_reviews = []\n",
    "X_values = []\n",
    "X_row_indices = []\n",
    "X_col_indices = []\n",
    "Y = []\n",
    "    \n",
    "pFiles = os.listdir(\"imdb_dataset/test/pos\")\n",
    "nFiles = os.listdir(\"imdb_dataset/test/neg\")\n",
    "\n",
    "#Read positive files\n",
    "for i in range(len(pFiles)):\n",
    "    f = pFiles[i]\n",
    "    lines = \"\"\n",
    "    for line in open(\"imdb_dataset/test/pos/%s\" % f, encoding=\"utf8\"):\n",
    "        lines += line\n",
    "        wordCounts = Counter([getID(w.lower()) for w in line.split(\" \")])\n",
    "        for (wordId, count) in wordCounts.items():\n",
    "            if wordId >= 0:\n",
    "                X_row_indices.append(i)\n",
    "                X_col_indices.append(wordId)\n",
    "                X_values.append(count)\n",
    "    Y.append(+1.0)\n",
    "    X_reviews.append(lines)\n",
    "    \n",
    "#Read negative files\n",
    "for i in range(len(nFiles)):\n",
    "    f = nFiles[i]\n",
    "    lines = \"\"\n",
    "    for line in open(\"imdb_dataset/test/neg/%s\" % f, encoding=\"utf8\"):\n",
    "        lines += line\n",
    "        wordCounts = Counter([getID(w.lower()) for w in line.split(\" \")])\n",
    "        for (wordId, count) in wordCounts.items():\n",
    "            if wordId >= 0:\n",
    "                X_row_indices.append(len(pFiles)+i)\n",
    "                X_col_indices.append(wordId)\n",
    "                X_values.append(count)\n",
    "    Y.append(-1.0)\n",
    "    X_reviews.append(lines)\n",
    "    \n",
    "#Create a sparse matrix in csr format\n",
    "X_test = csr_matrix((X_values, (X_row_indices, X_col_indices)), shape=(max(X_row_indices)+1, len(words)))        \n",
    "Y_test = np.asarray(Y)\n",
    "\n",
    "#Randomly shuffle\n",
    "index = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(index)\n",
    "X_test = X_test[index,:]\n",
    "Y_test = Y_test[index]\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Eval:\n",
    "    def __init__(self, pred, gold):\n",
    "        self.pred = pred\n",
    "        self.gold = gold\n",
    "        \n",
    "    def Accuracy(self):\n",
    "        return np.sum(np.equal(self.pred, self.gold)) / float(len(self.gold))\n",
    "    \n",
    "    def Recall(self):\n",
    "        return recall_score(self.gold,self.pred)\n",
    "    \n",
    "    def Precision(self):\n",
    "         return precision_score(self.gold,self.pred)\n",
    "     \n",
    "    def PvRcurve(self):\n",
    "        average_precision = average_precision_score(self.gold, self.pred)\n",
    "        precision, recall, _ = precision_recall_curve(self.gold, self.pred)\n",
    "\n",
    "        plt.step(recall, precision, color='b', alpha=0.2,where='post')\n",
    "        plt.fill_between(recall, precision, step='post', alpha=0.2,color='b')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, dataX, dataY, ALPHA=1.0):\n",
    "        self.ALPHA = ALPHA\n",
    "        self.dataX = dataX\n",
    "        self.dataY = dataY\n",
    "        self.vocab_len = 1\n",
    "        self.count_positive = 1\n",
    "        self.count_negative = 1\n",
    "        self.num_positive_reviews = 1\n",
    "        self.num_negative_reviews = 1\n",
    "        self.total_positive_words = 1\n",
    "        self.total_negative_words = 1\n",
    "        self.P_positive = 1\n",
    "        self.P_negative = 1\n",
    "        self.deno_pos = 1\n",
    "        self.deno_neg =1\n",
    "        self.pos_words=[]\n",
    "        self.neg_words=[]\n",
    "        self.train(dataX,dataY)\n",
    "\n",
    "    # Train model - X are instances, Y are labels (+1 or -1)\n",
    "    # X and Y are sparse matrices\n",
    "    def train(self, X, Y):\n",
    "        #TODO: Estimate Naive Bayes model parameters\n",
    "        #calculate frequency parameters to be used in PredictLabel to calculate probabilities\n",
    "        positive_indices = np.argwhere(Y == 1.0).flatten()\n",
    "        negative_indices = np.argwhere(Y == -1.0).flatten()\n",
    "        \n",
    "        self.num_positive_reviews = sum([1 if i==1 else 0 for i in Y])\n",
    "        self.num_negative_reviews = sum([1 if i==-1 else 0 for i in Y])\n",
    "        self.count_positive = np.zeros(X.shape[1])\n",
    "        self.count_negative = np.zeros(X.shape[1])\n",
    "        self.total_positive_words = np.sum(X[positive_indices,:])\n",
    "        self.total_negative_words = np.sum(X[negative_indices,:])\n",
    "        \n",
    "        #For smoothing\n",
    "        self.deno_pos = self.total_positive_words + self.ALPHA*X.shape[1]\n",
    "        self.deno_neg = self.total_negative_words + self.ALPHA*X.shape[1]\n",
    "        \n",
    "        rows,columns = X.nonzero()\n",
    "        for i,j in zip(rows,columns):\n",
    "            if self.dataY[i]==1:\n",
    "                    self.count_positive[j]+=X[i,j]\n",
    "            else:\n",
    "                    self.count_negative[j]+=X[i,j]\n",
    "        self.count_positive = (self.count_positive + self.ALPHA)\n",
    "        self.count_negative = (self.count_negative + self.ALPHA)        \n",
    "        #above 2 arrays give total frequencies for each word in each class\n",
    "        return\n",
    "\n",
    "    # Predict labels for instances X\n",
    "    # Return: Sparse matrix Y with predicted labels (+1 or -1)\n",
    "    def PredictLabel(self, X, threshold=0.5):\n",
    "        #Calculate P(W|C) and P(C) to get P(C|W) by doing a logsum\n",
    "        self.P_positive = log(self.num_positive_reviews)-(log(self.num_positive_reviews)+log(self.num_negative_reviews))\n",
    "        self.P_negative = log(self.num_negative_reviews)-(log(self.num_positive_reviews)+log(self.num_negative_reviews))\n",
    "        pred_labels = []\n",
    "        w=X.shape[1]\n",
    "        sh = X.shape[0]\n",
    "        for i in range(sh):\n",
    "           #checks if the value of the data is zero or not if not then proceed\n",
    "            z = X[i].nonzero()\n",
    "            positive_sum = self.P_positive\n",
    "            negative_sum = self.P_negative\n",
    "            for j in range(len(z[0])):\n",
    "                # Look at each feature\n",
    "                row_index = i\n",
    "                col_index = z[1][j]\n",
    "                occurrence = X[row_index, col_index]\n",
    "                P_pos = log(self.count_positive[col_index]) - log(self.deno_pos)\n",
    "                positive_sum = positive_sum + occurrence * P_pos\n",
    "                P_neg = log(self.count_negative[col_index]) - log(self.deno_neg)\n",
    "                negative_sum = negative_sum + occurrence * P_neg\n",
    "            probValue = exp(positive_sum - self.LogSum(positive_sum, negative_sum))\n",
    "            if probValue > threshold:            # Predict positive\n",
    "                pred_labels.append(1.0)\n",
    "            else:               # Predict negative\n",
    "                pred_labels.append(-1.0)  \n",
    "                \n",
    "        return pred_labels\n",
    "\n",
    "\n",
    "    def LogSum(self, logx, logy):   \n",
    "        # TO DO: Return log(x+y), avoiding numerical underflow/overflow.\n",
    "        m = max(logx, logy)   \n",
    "        #print(m)\n",
    "        return m + log(exp(logx - m) + exp(logy - m))\n",
    "\n",
    "    # Predict the probability of each indexed review in sparse matrix text\n",
    "    # of being positive\n",
    "    # Prints results\n",
    "    def PredictProb(self, testX, testY, indexes):\n",
    "         \n",
    "        for i in indexes:\n",
    "            predicted_label = 0\n",
    "            z = testX[i].nonzero()\n",
    "            positive_sum = self.P_positive\n",
    "            negative_sum = self.P_negative\n",
    "\n",
    "            for j in range(len(z[0])):\n",
    "                row_index = i\n",
    "                col_index = z[1][j]\n",
    "                occurrence = testX[row_index, col_index]\n",
    "                P_pos = log(self.count_positive[col_index])\n",
    "                positive_sum = positive_sum + occurrence * P_pos\n",
    "                P_neg = log(self.count_negative[col_index])\n",
    "                negative_sum = negative_sum + occurrence * P_neg\n",
    "                \n",
    "\n",
    "            predicted_prob_positive = exp(positive_sum - self.LogSum(positive_sum, negative_sum))\n",
    "            predicted_prob_negative = exp(negative_sum - self.LogSum(positive_sum, negative_sum))\n",
    "\n",
    "            if positive_sum > negative_sum:\n",
    "                predicted_label=1.0\n",
    "            else:\n",
    "                predicted_label=-1.0\n",
    "\n",
    "            print (testY[i], testX_reviews[i],predicted_label)\n",
    "\n",
    "            # TODO: Comment the line above, and uncomment the line below\n",
    "            #print(test.Y[i], predicted_label, predicted_prob_positive, predicted_prob_negative)\n",
    "\n",
    "    # Evaluate performance on test data \n",
    "    #Gives Accuracy, Precision and Recall for different values of a threshold\n",
    "    def Eval(self):\n",
    "        Y_pred = self.PredictLabel(X_test)\n",
    "        ev = Eval(Y_pred, Y_test)\n",
    "        print(\"For Positive Class:\")\n",
    "        print(\"Test Accuracy: \",ev.Accuracy())\n",
    "        print(\"Test Recall: \",ev.Recall())\n",
    "        print(\"Test Precision: \",ev.Precision())\n",
    "        print(\"\\n\")\n",
    "        print(\"For Negative Class:\")\n",
    "        ev_neg = Eval([1 if i == -1 else -1 for i in Y_pred], [1 if i == -1 else -1 for i in Y_test])\n",
    "        print(\"Test Accuracy: \",ev_neg.Accuracy())\n",
    "        print(\"Test Recall: \",ev_neg.Recall())\n",
    "        print(\"Test Precision: \",ev_neg.Precision())\n",
    "        probality_threshold=[0.2,0.4,0.6,0.8]\n",
    "        Precision=[]\n",
    "        Recall=[]\n",
    "        Precision.append(ev.Precision())\n",
    "        Recall.append(ev.Recall())\n",
    "        length=len(probality_threshold)\n",
    "        for i in range(0,length):\n",
    "            Y_pred = self.PredictLabel(X_test,probality_threshold[i])\n",
    "            ev = Eval(Y_pred, Y_test)\n",
    "            Precision.append(ev.Precision())\n",
    "            Recall.append(ev.Recall())\n",
    "        plt.plot(Precision,Recall)\n",
    "        plt.ylabel('Recall')\n",
    "        plt.xlabel('Precision')\n",
    "        #plt.ylim([0.0, 1.05])\n",
    "        #plt.xlim([0.0, 1.0])\n",
    "        plt.title('2-class Precision-Recall curve')\n",
    "        plt.show()\n",
    "        #above statements show a Precision vs Recall graph\n",
    "\n",
    "    #TO-DO: give top 20 positive and negative words based on their log odds\n",
    "    #words with the max difference value would be part of this list\n",
    "    def Features(self):\n",
    "        pos_diff=np.zeros(self.data.X.shape[1])\n",
    "        neg_diff=np.zeros(self.data.X.shape[1])\n",
    "        for j in range(len(self.count_positive)):\n",
    "                P_pos = log(self.count_positive[j]) - log(self.deno_pos)\n",
    "                P_neg = log(self.count_negative[j]) - log(self.deno_neg)\n",
    "                pos_diff[j]=(P_pos-P_neg)\n",
    "                #neg_diff[j]=(P_neg-P_pos)*(self.count_negative[j]-self.count_positive[j])\n",
    "        print(\"Top 20 Positive words with their weights:\")        \n",
    "        pos_index=pos_diff.argsort()[-20:][::-1]\n",
    "        for j in pos_index:\n",
    "            print(\"j:\",self.data.vocab.GetWord(j),\" \",pos_diff[j])\n",
    "        \n",
    "        print(\"Top 20 Negative words with their weights:\")\n",
    "        neg_index=pos_diff.argsort()[:20]\n",
    "        for j in neg_index:\n",
    "            print(\"j:\",self.data.vocab.GetWord(j),\" \",pos_diff[j])\n",
    "            #Alternate better approach below: needs dubugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Parameters\n",
      "Evaluating\n",
      "For Positive Class:\n",
      "Test Accuracy:  0.80132\n",
      "Test Recall:  0.7416\n",
      "Test Precision:  0.8421913327882257\n",
      "\n",
      "\n",
      "For Negative Class:\n",
      "Test Accuracy:  0.80132\n",
      "Test Recall:  0.86104\n",
      "Test Precision:  0.769170299435432\n"
     ]
    }
   ],
   "source": [
    "    print(\"Computing Parameters\")\n",
    "    nb = NaiveBayes(X_train, Y_train)\n",
    "    print(\"Evaluating\")\n",
    "    nb.Eval()\n",
    "    nb.PredictProb(X_test,Y_test,range(10))\n",
    "    nb.Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
